{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1          Sarvjeet Singh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1:\n",
    "Machine Learning is the study of algorithms so that program can take independent decisions without explicitly programmed to do so. It is the process of making intelligent machines, so that they can learn through their own experience and get better automatically without being programmed by a programmer. It is considered as the subset of artificial intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2: \n",
    "The main difference between the supervised and unsupervised machine learning algorithm is that in supervised machine laerning algorithm we provide input variables ( features) as well as the prediction varibale or outcome variable which we want to predict. While in unsupervised machine learning we provide only input data, we don't have any labels or prediction variale in  this case. Unsupervised Machine learning is used to find the hidden patterns in the data. \n",
    "Example of Supervised Learning:\n",
    "1. Student Grade prediction\n",
    "2. Credit  fraud detection\n",
    "3. Image classification\n",
    "\n",
    "Examples of Unsupervised Machine Learning:\n",
    "1. Anomaly detection\n",
    "2. Neural Networks\n",
    "3. Classification of customers on the basis of their purchase\n",
    "\n",
    "Supervised Machine learning Algorithms:\n",
    "1. Regression\n",
    "2. Classification \n",
    "3. Support Vector Machine\n",
    "\n",
    "Unsupervised Machine Learning Algorithms:\n",
    "1. Clustering\n",
    "2. Neural Networks\n",
    "3. K-Nearest Neighbour\n",
    "4. Apriori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3:\n",
    "If we use our whole data set for training and then testing purpose, we will get a model which is very baised on that particular data set and may not be very useful in the actual production and may not be able to use in real life.\n",
    "To solve this problem we split our dataset into 2 or 3 parts. First part is train set, second validation set, third test set.\n",
    "\n",
    "### Validation data \n",
    "Validation data is used for tuning the parameters of the model after training, before finalization of the model.\n",
    "\n",
    "### Test data \n",
    "Test data is used for finally evaluating the model. A model is useful only if it perform well on the test data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 4:\n",
    "Steps involved in pre-processing are\n",
    "1. Gathering the data\n",
    "First step is to gather the data. \n",
    "\n",
    "2. Import All necessary libraries\n",
    "For working on the data set and doing various tasks, one need various libraries like Sklear, Matplotlib, Numpy, Pandas etc.\n",
    "Using import keyword.\n",
    "\n",
    "3. Import Data set\n",
    "Import the data set into your notebook. \n",
    "\n",
    "4. Handling missing values or abnormal values\n",
    "Often datasets contain missing values or values of different types in a variable. Hence it is required to either remove that variable or fill these values with the mean or median of the values, sometimes we often use most frequently occured values to fill the missing places. We can check missing values as\n",
    "#### dataframe.isna().sum()\n",
    "\n",
    "5. Split our data set into train and test data\n",
    "Spliting data set is required to enhance the model and improve the model by tuning it's various parameters, to make a useful model\n",
    "\n",
    "6. Feature Scaling\n",
    "Feature Scaling is the standardization of the various fetaures (independent variables) into a specific range. It limits the range of our features.\n",
    "we can use two methods for feature scaling:\n",
    " 6.1. Standardization\n",
    " 6.2. Normalization\n",
    " \n",
    " Almost every dataset contain noise, missing values. hence it cannot be direclty use for predictions, so data preprocessing cleans data and make it ready for the machine learning purposes.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 5:\n",
    "#### Continuous \n",
    "Continuous variables are the one in which the variable can take any value between the maximum and mnimum values.\n",
    "\n",
    "Methods to handle continous variables:\n",
    "1. Binning\n",
    "2. Normalization\n",
    "3. Transformations for Skewed Distribution\n",
    "\n",
    "#### Discrete Variable\n",
    "Discrete variables are the one which are obtained by counting, here each data point has a unique value. \n",
    "we can use Standardization for the exploration of discrete variable.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 6:\n",
    "1. It is a Histogram.\n",
    "2. Continous variable.\n",
    "3. steps for preprocessing\n",
    " 3.1. we need to trasnform it to scalable variables.\n",
    " 3.2. Standardize the data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
